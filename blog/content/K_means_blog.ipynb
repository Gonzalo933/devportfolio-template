{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='k-means'></a>\n",
    "I want to start this little machine learning blog with a simple but effective algorithm to identify [clusters](https://en.wikipedia.org/wiki/Cluster_analysis) in data. For those that are not familiar with the concept that means finding groups that could be used for finding  relations in all the observations that belongs to a cluster or just to find the number of different clusters in some data, for example, in a social network.\n",
    "\n",
    "As this is the first post I will explain some things about notations, that are also on the [introduction](introduction.html#notation):\n",
    "\n",
    "- All vectorial variables are written in bold ex: $ \\mathbf{x} , \\mathbf{y} $.\n",
    "\n",
    "- All vectors are always column vectors (unless otherwise stated) so a $ x $ vector of size $ N $ would be written as: \n",
    "\n",
    "  $\\mathbf{x} = \\begin{bmatrix}  x_1 \\\\ x_2 \\\\ \\vdots\\\\ x_n \\end{bmatrix} $  or $ \\mathbf{x} = \\begin{bmatrix}  x_1, \\ x_2,\\ \\dots, \\ x_n \\end{bmatrix}^T $\n",
    "\n",
    "- Matrices are written both in bold and uppercase, ex: $ \\mathbf{X},  \\mathbf{Y} $\n",
    "\n",
    "  so if the size of $ \\mathbf{X} $ is $N\\times D$: \n",
    "\n",
    "   $  \\mathbf{X} = \\begin{bmatrix}    x_{11} & \\dots  & x_{1d} \\\\    x_{21} & \\dots  & x_{2d} \\\\    \\vdots & \\ddots & \\vdots \\\\    x_{n1} & \\dots  & x_{nd}\\end{bmatrix} $\n",
    "\n",
    "\n",
    "K-means is an algorithm to partition $ N $ observations of a random D-dimensional variable $ \\mathbf{X}$ (which we will refer as the dataset) into $ \\mathit{K} $ clusters, the value of $ \\mathit{K} $ is given to us. We can think of a cluster as a group of points (from the dataset) whose distance between each other are small compared to distances to points outside of the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The Dataset\n",
    "\n",
    "Lets study first the dataset we are going to work with. I chose one called old faithful, and if you ever read a book about machine learning you will study it deeply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'old_faithful.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6e9c39d00a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'old_faithful.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Round all data to two decimal places\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Drop 1st column, we now have: 'eruptions' and 'waiting'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gonzalo/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gonzalo/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gonzalo/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gonzalo/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gonzalo/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'old_faithful.csv' does not exist"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#loading the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns # Nice plots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "df = pd.read_csv('old_faithful.csv')\n",
    "df.round(2) # Round all data to two decimal places\n",
    "df.drop(df.columns[0], axis=1, inplace=True) # Drop 1st column, we now have: 'eruptions' and 'waiting'\n",
    "df = (df - df.mean()) / df.std() # Standarize the data (make each variable zero mean and unit standard deviation)\n",
    "df.plot(x='eruptions', y='waiting', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using [pandas](http://pandas.pydata.org/pandas-docs/stable/) to work with the dataset.\n",
    "As you can see we are working in two dimensions (waiting time and eruptions), the code above rounds all the data to two decimals places, then \"deletes\" the first column (we don't need it, is just an index) and last it [standardizes](https://stats.idre.ucla.edu/stata/faq/how-do-i-standardize-variables-in-stata/) the data.\n",
    "and we can already clearly see two clusters. If you don't see the clusters that I'm talking about I can help you with my ultra MS paint&trade; skills. \n",
    "\n",
    "![Two Clusters, see?](images\\dont_you_see_the_clusters.png \"Two Clusters, see?\")\n",
    "\n",
    "## Formalizing the dataset\n",
    "\n",
    "Lets start by naming some of the variables that we are going to use.\n",
    "\n",
    "We are working in two dimensions right? and we [stated](#k-means) that we are working with a dataset $ \\mathbf{X} $ of size $ N \\times D $. Here we have our first variables:\n",
    "\n",
    "$ \\mathbf{X} $ is the dataset matrix, one row for each observation, and each observation has two values (waiting and eruptions) so we got that the size of the matrix is:\n",
    "\n",
    "$ N = 272 $ (trust me on this one) $\\times$ $ D = 2 $\n",
    "\n",
    "Now we need to choose in how many cluster we want to divide the data, we already saw that there are probably two clusters, so lets assume that, set $ K = 2 $.\n",
    "\n",
    "we can now add that to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.as_matrix()\n",
    "N = X.shape[0]\n",
    "K = 2\n",
    "D = X.shape[1] # number of dimensions of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bs}[1]{\\boldsymbol{#1}}$\n",
    "$\\newcommand{\\Epsilon}[0]{\\mathcal{E}}$\n",
    "We are still missing some things before we can start. If we are going to create clusters we need to define the centers of the clusters, for example we can store one of the centers in a variable $ \\bs{\\mu} $ that would need to be of size D. If we have K clusters then our variable $ \\bs{\\mu} $ (it's uppercase even if it doesn't look like it) would be of size $ K\\times D $.\n",
    "\n",
    "We also need something to describe to which cluster is each point assigned. We can do that with another matrix $R$ of size $N\\times K$, the matrix means something like this: We have a row for each of the points ($N$) and each row has $1$ in the position of the cluster that the point is assigned to and $0$ on the rest of the positions.\n",
    "\n",
    "Here is an example to understand it better:\n",
    "\n",
    "$$ \\underset{N\\times K}{\\mathbf{R}} = \\begin{bmatrix}\n",
    "    1 & 0\\\\\n",
    "    0 & 1\\\\\n",
    "    1 & 0 \\\\\n",
    "    \\vdots & \\vdots\n",
    "    \\end{bmatrix} $$\n",
    "    \n",
    "It says that the first datapoint is assigned to the first cluster, the second point to the second one and the third one to the first cluster. (note that **each point can only be assigned to one and only one of the clusters**).\n",
    "\n",
    "\n",
    "\n",
    "## Math time\n",
    "\n",
    "Now we need and algorithm that give us the best choices for both the assignments and the centers of the clusters but\n",
    "if we expect to implement an algorithm we are going to need math, don't worry this ones aren't complicated.\n",
    "\n",
    "We can start with a function $J()$ that tell us how good or how bad our algorithm is doing, we call that *the objective funcion* or *the error function* and we can use something as simple as this:\n",
    "$$ J = \\sum_{n=1}^{N}\\sum_{k=1}^{K} r_{nk}|| \\bs{x_n} - \\bs{\\mu_k} ||^2 \\tag{1}$$ \n",
    "\n",
    "This represents the [sum of the squares](https://en.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance) of the distances of each point to the center of the assigned cluster. \n",
    "\n",
    "We can break the equation to understand it better (or you can skip the exaplanation if you feel confident).\n",
    "we are going to loop through all the points and all the clusters that means that if $r_{nk} = 1 $ we are in a point $ x_n $ that belongs to the cluster $ k $ and we calculate the distance from $ \\bs{x_n} $ to $ \\bs{\\mu_k} $ and we add that value to our total error (we are in a summation). If $r_{nk} = 0 $ the point doesn't belong to the cluster and we don't even need to calculate anything, we sum 0 to the error.\n",
    "\n",
    "Okay so now we have some function that we can call with some parameters and give us a number that tell us if we are in the correct path to solve the problem, we obviously want a small error, so we want to minimize $J$.\n",
    "\n",
    "take a look at $ (1) $ again, we see that it depends on $\\bs{X}, \\mu $ and $\\bs{R}$, and of that list we can only change $ R $ and $ \\mu $ so our goal now is to find the values of those variables that minimize $J$, we can do that in two phases, first minimizing $J$ with respect to $\\bs{R}$ fixing the other parameters and then with respect to $\\mu $ fixing the other parameters.\n",
    "\n",
    "## Optimize $R$\n",
    "\n",
    "because $(1)$ is a linear function of $r_{nk}$ we can optimize for each datapoint esparately by doing this:\n",
    "\n",
    "$$ \n",
    "r_{nk} = \n",
    "\\begin{cases} \n",
    "    1 & \\text{if $k$ = arg  $min_j || \\bs{x_n} - \\bs{\\mu_j} ||^2 $} \\\\\n",
    "    0 & \\text{otherwise} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "intuitively that means that we calculate all distances to all centers  of the $ n^{th} $ point and assign the $n^{th}$ data point to the closest cluster centre (the one with less distance $|| \\bs{x_n} - \\bs{\\mu_j} ||^2$.\n",
    "\n",
    "now to optimize the cluster centres $\\bs{\\mu_k} $ we can set the derivative of $(1)$ to zero with respect to $\\bs{\\mu_k} $ and we have\n",
    "$$\n",
    "2\\sum_{n=1}^{N}r_{nk}(\\bs{x_n} - \\bs{\\mu_k}) = 0\n",
    "$$\n",
    "$$\n",
    "\\bs{\\mu_k} = \\frac{\\sum_{n=1}^{N} r_{nk}\\bs{x_n}}{\\sum_{n=1}^{N}r_{nk}} \\tag{2}\n",
    "$$\n",
    "\n",
    "eq $(2)$ just says: \n",
    "$$ \n",
    "\\bs{\\mu_k} = \\frac{\\text{sum of the values of points assigned to cluster k}}{\\text{number of points assigned to cluster k}}\n",
    "$$\n",
    "\n",
    "[sounds familiar?](https://en.wikipedia.org/wiki/Arithmetic_mean), well that is why is called K-means. Now we can start with the fun part and code all of this.\n",
    "\n",
    "Note: the trick to make an algortihm fast in machine learning is to vectorize all the operations instead of looping through the values. we could code the eq. $(1)$ as two for loops but we will be making our algorithm $O(N^2)$ if we code it the way is done below the time complexity is $O(N)$ but it is equivalent. we also converted the matrix $R$ in a vector called `labels` of size $N$ which each value can take any number from $ 0 $ to $ K - 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Error function\n",
    "def J(distances, labels):\n",
    "    error = 0\n",
    "    for n, distance in enumerate(distances):\n",
    "        error += distance[labels[n]] # we could have made it even more vectorial, but it's not worth it\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start initializing the cluster centers at random (or better, choose some acceptable centers).\n",
    "We then calculate the distances matrix `squared_distances` (corresponding to $|| \\bs{x_n} - \\bs{\\mu_k} ||^2  $ and to which cluster are closer, called `labels`.\n",
    "\n",
    "I also printed the error (this is optative but helps so we know if we are doing it right, because it should be decreasing in each iteration).\n",
    "\n",
    "This is the final code then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12, 6] #Bigger images\n",
    "\n",
    "MU = np.array([[-2.0, 2.0], [1.5, -2.5]])\n",
    "max_iterations = 7\n",
    "\n",
    "for _ in range(max_iterations):\n",
    "    # Calculate distance from each datapoint to each cluster center\n",
    "    squared_distances = cdist(X, MU, 'sqeuclidean') # NxK matrix\n",
    "    labels = np.argmin(squared_distances, axis=1) # Nx1 and each value tell us which index is associated which each datapoint\n",
    "\n",
    "    # calculate the means of all datapoints assigned to each cluster (Kx1)\n",
    "    for k in range(K):\n",
    "        # points_of_cluster k -> X[labels == k]\n",
    "        # new centers\n",
    "        MU[k] = np.mean(X[labels == k], axis=0) #out=MU[k]    \n",
    "    # Error should be decreasing\n",
    "    print(\"Current error: {0}\".format(J(squared_distances, labels)))\n",
    "    # end of iteration, now we have labels of which cluster the datapoint is associated and the new cluster locations (MU)\n",
    "#plt.figure()\n",
    "df = df.assign(labels=labels)\n",
    "df.plot.scatter(x='eruptions', y='waiting', c='labels', colormap=cmx.coolwarm, colorbar=False)\n",
    "\n",
    "plt.scatter(MU[:,0], MU[:,1], marker='o', linewidths=5, color=['c', '#DA0AF0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the two clusters (in red and blue), corresponding to plotting the matrix $\\bs{X}$ and choosing the color according to the value of `labels[N]` (if is equal to 0 or 1).\n",
    "\n",
    "The cluster centers are also plotted (in magenta and cyan) and effectively we see that it correspond to the mean value of all the points of each cluster.\n",
    "\n",
    "Last, we can check that in each iteration the error was decreasing, I can plot that too so we can also see that there is a point in which it stops changing (we say that \"the algorithm has converged\")\n",
    "\n",
    "![Plotting Error](images\\error_minimizing.png \"Error reduction\")\n",
    "\n",
    "\n",
    "I also created a gif ilustrating each iteration of the algorithm. I plotted two figures for each iteration, one showing the updates of the labels in which each point get assigned to a center deopending on in which side of the line (that's the perpendicular bisector between the centers) is at and another update showing the new cluster centers.\n",
    "\n",
    "![Iterations](images\\iterations.gif \"Iterations\")\n",
    "\n",
    "And that was pretty much everything, you can [download the source]({attach}/code/k_means.zip) and soon I will upload another blog entry with an interesting application of K-means to image compression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### References\n",
    "\n",
    "Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Cluster_analysis\n",
    "\n",
    "https://en.wikipedia.org/wiki/K-means_clustering\n",
    "\n",
    "https://stats.idre.ucla.edu/stata/faq/how-do-i-standardize-variables-in-stata/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Arithmetic_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
